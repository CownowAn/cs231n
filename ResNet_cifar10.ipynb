{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet_cifar10.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDhxI3uFpulrL47uHdbWk9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":119,"metadata":{"id":"6fUP_KBvz06B","executionInfo":{"status":"ok","timestamp":1642331805550,"user_tz":-540,"elapsed":289,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"outputs":[],"source":["from torchsummary import summary\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","from torchvision import models\n","from torchvision import utils\n","\n","import numpy as np"]},{"cell_type":"code","source":["USE_GPU = True\n","dtype = torch.float32 # We will be using float throughout this tutorial.\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss.\n","print_every = 100\n","print('using device:', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XW55r7WAcY-E","executionInfo":{"status":"ok","timestamp":1642331805836,"user_tz":-540,"elapsed":4,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"af4e23e9-3ff6-49da-9fc5-ea463736abb6"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]}]},{"cell_type":"code","source":["NUM_TRAIN = 49000\n","\n","transform_train = T.Compose([\n","                      T.Resize(256), T.CenterCrop(224),\n","                      # T.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n","                      T.ToTensor(), \n","                      T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","                      ])\n","\n","transform_test = T.Compose([\n","                      T.ToTensor(), \n","                      T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","                      ])\n","\n","cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n","                             transform=transform_train)\n","loader_train = DataLoader(cifar10_train, batch_size=64, \n","                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n","\n","cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n","                           transform=transform_train)\n","loader_val = DataLoader(cifar10_val, batch_size=64, \n","                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n","\n","cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n","                            transform=transform_test)\n","loader_test = DataLoader(cifar10_test, batch_size=64)"],"metadata":{"id":"Pv4RAgb6z_6L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642339774027,"user_tz":-540,"elapsed":2810,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"e8b03848-9255-45b8-9e99-6e72572dce0a"},"execution_count":169,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["!pip install torchsummary"],"metadata":{"id":"emVNucfC0Bsw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642339778029,"user_tz":-540,"elapsed":2614,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"98e9b29c-d6c2-4929-dfcc-87d80e32e6c1"},"execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["def check_accuracy(loader, model):\n","    if loader.dataset.train:\n","        print('Checking accuracy on validation set')\n","    else:\n","        print('Checking accuracy on test set')   \n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","            scores = model(x)\n","            _, preds = scores.max(1)\n","            num_correct += (preds == y).sum()\n","            num_samples += preds.size(0)\n","        acc = float(num_correct) / num_samples\n","        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n","        return acc"],"metadata":{"id":"l3riOVZEeqoa","executionInfo":{"status":"ok","timestamp":1642339778029,"user_tz":-540,"elapsed":3,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, scheduler, epochs=1):\n","    model = model.to(device=device)\n","    for e in range(epochs):\n","        print(f\"--------------------  Epoch: {e+1}  --------------------\")\n","        for t, (x, y) in enumerate(loader_train):\n","            model.train()  # put model to training mode\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","\n","            scores = model(x)\n","            loss = F.cross_entropy(scores, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if t % print_every == 0:\n","                print('Epoch %d' % (e+1))\n","                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n","                val_acc = check_accuracy(loader_val, model)\n","                print()\n","\n","        scheduler.step(val_acc)"],"metadata":{"id":"O8NYWO0ueG4Z","executionInfo":{"status":"ok","timestamp":1642339778773,"user_tz":-540,"elapsed":2,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["model = models.resnet101(pretrained=True)\n","num_features = model.fc.in_features\n","model.fc = torch.nn.Linear(num_features, 10)\n","model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n","model = model.to(device)\n","print(model.conv1)\n","summary(model, (3, 32, 32))"],"metadata":{"id":"tSAWg30J0GdM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642339781567,"user_tz":-540,"elapsed":1355,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"c3a127c5-f67d-4501-b6d7-9b61ce2ce509"},"execution_count":173,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           1,728\n","       BatchNorm2d-2           [-1, 64, 16, 16]             128\n","              ReLU-3           [-1, 64, 16, 16]               0\n","         MaxPool2d-4             [-1, 64, 8, 8]               0\n","            Conv2d-5             [-1, 64, 8, 8]           4,096\n","       BatchNorm2d-6             [-1, 64, 8, 8]             128\n","              ReLU-7             [-1, 64, 8, 8]               0\n","            Conv2d-8             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-9             [-1, 64, 8, 8]             128\n","             ReLU-10             [-1, 64, 8, 8]               0\n","           Conv2d-11            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-12            [-1, 256, 8, 8]             512\n","           Conv2d-13            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-14            [-1, 256, 8, 8]             512\n","             ReLU-15            [-1, 256, 8, 8]               0\n","       Bottleneck-16            [-1, 256, 8, 8]               0\n","           Conv2d-17             [-1, 64, 8, 8]          16,384\n","      BatchNorm2d-18             [-1, 64, 8, 8]             128\n","             ReLU-19             [-1, 64, 8, 8]               0\n","           Conv2d-20             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-21             [-1, 64, 8, 8]             128\n","             ReLU-22             [-1, 64, 8, 8]               0\n","           Conv2d-23            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-24            [-1, 256, 8, 8]             512\n","             ReLU-25            [-1, 256, 8, 8]               0\n","       Bottleneck-26            [-1, 256, 8, 8]               0\n","           Conv2d-27             [-1, 64, 8, 8]          16,384\n","      BatchNorm2d-28             [-1, 64, 8, 8]             128\n","             ReLU-29             [-1, 64, 8, 8]               0\n","           Conv2d-30             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-31             [-1, 64, 8, 8]             128\n","             ReLU-32             [-1, 64, 8, 8]               0\n","           Conv2d-33            [-1, 256, 8, 8]          16,384\n","      BatchNorm2d-34            [-1, 256, 8, 8]             512\n","             ReLU-35            [-1, 256, 8, 8]               0\n","       Bottleneck-36            [-1, 256, 8, 8]               0\n","           Conv2d-37            [-1, 128, 8, 8]          32,768\n","      BatchNorm2d-38            [-1, 128, 8, 8]             256\n","             ReLU-39            [-1, 128, 8, 8]               0\n","           Conv2d-40            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-41            [-1, 128, 4, 4]             256\n","             ReLU-42            [-1, 128, 4, 4]               0\n","           Conv2d-43            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n","           Conv2d-45            [-1, 512, 4, 4]         131,072\n","      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n","             ReLU-47            [-1, 512, 4, 4]               0\n","       Bottleneck-48            [-1, 512, 4, 4]               0\n","           Conv2d-49            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-50            [-1, 128, 4, 4]             256\n","             ReLU-51            [-1, 128, 4, 4]               0\n","           Conv2d-52            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-53            [-1, 128, 4, 4]             256\n","             ReLU-54            [-1, 128, 4, 4]               0\n","           Conv2d-55            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n","             ReLU-57            [-1, 512, 4, 4]               0\n","       Bottleneck-58            [-1, 512, 4, 4]               0\n","           Conv2d-59            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-60            [-1, 128, 4, 4]             256\n","             ReLU-61            [-1, 128, 4, 4]               0\n","           Conv2d-62            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-63            [-1, 128, 4, 4]             256\n","             ReLU-64            [-1, 128, 4, 4]               0\n","           Conv2d-65            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n","             ReLU-67            [-1, 512, 4, 4]               0\n","       Bottleneck-68            [-1, 512, 4, 4]               0\n","           Conv2d-69            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-70            [-1, 128, 4, 4]             256\n","             ReLU-71            [-1, 128, 4, 4]               0\n","           Conv2d-72            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-73            [-1, 128, 4, 4]             256\n","             ReLU-74            [-1, 128, 4, 4]               0\n","           Conv2d-75            [-1, 512, 4, 4]          65,536\n","      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n","             ReLU-77            [-1, 512, 4, 4]               0\n","       Bottleneck-78            [-1, 512, 4, 4]               0\n","           Conv2d-79            [-1, 256, 4, 4]         131,072\n","      BatchNorm2d-80            [-1, 256, 4, 4]             512\n","             ReLU-81            [-1, 256, 4, 4]               0\n","           Conv2d-82            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-83            [-1, 256, 2, 2]             512\n","             ReLU-84            [-1, 256, 2, 2]               0\n","           Conv2d-85           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n","           Conv2d-87           [-1, 1024, 2, 2]         524,288\n","      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n","             ReLU-89           [-1, 1024, 2, 2]               0\n","       Bottleneck-90           [-1, 1024, 2, 2]               0\n","           Conv2d-91            [-1, 256, 2, 2]         262,144\n","      BatchNorm2d-92            [-1, 256, 2, 2]             512\n","             ReLU-93            [-1, 256, 2, 2]               0\n","           Conv2d-94            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-95            [-1, 256, 2, 2]             512\n","             ReLU-96            [-1, 256, 2, 2]               0\n","           Conv2d-97           [-1, 1024, 2, 2]         262,144\n","      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n","             ReLU-99           [-1, 1024, 2, 2]               0\n","      Bottleneck-100           [-1, 1024, 2, 2]               0\n","          Conv2d-101            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-102            [-1, 256, 2, 2]             512\n","            ReLU-103            [-1, 256, 2, 2]               0\n","          Conv2d-104            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-105            [-1, 256, 2, 2]             512\n","            ReLU-106            [-1, 256, 2, 2]               0\n","          Conv2d-107           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n","            ReLU-109           [-1, 1024, 2, 2]               0\n","      Bottleneck-110           [-1, 1024, 2, 2]               0\n","          Conv2d-111            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-112            [-1, 256, 2, 2]             512\n","            ReLU-113            [-1, 256, 2, 2]               0\n","          Conv2d-114            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-115            [-1, 256, 2, 2]             512\n","            ReLU-116            [-1, 256, 2, 2]               0\n","          Conv2d-117           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n","            ReLU-119           [-1, 1024, 2, 2]               0\n","      Bottleneck-120           [-1, 1024, 2, 2]               0\n","          Conv2d-121            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-122            [-1, 256, 2, 2]             512\n","            ReLU-123            [-1, 256, 2, 2]               0\n","          Conv2d-124            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-125            [-1, 256, 2, 2]             512\n","            ReLU-126            [-1, 256, 2, 2]               0\n","          Conv2d-127           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n","            ReLU-129           [-1, 1024, 2, 2]               0\n","      Bottleneck-130           [-1, 1024, 2, 2]               0\n","          Conv2d-131            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-132            [-1, 256, 2, 2]             512\n","            ReLU-133            [-1, 256, 2, 2]               0\n","          Conv2d-134            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-135            [-1, 256, 2, 2]             512\n","            ReLU-136            [-1, 256, 2, 2]               0\n","          Conv2d-137           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n","            ReLU-139           [-1, 1024, 2, 2]               0\n","      Bottleneck-140           [-1, 1024, 2, 2]               0\n","          Conv2d-141            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-142            [-1, 256, 2, 2]             512\n","            ReLU-143            [-1, 256, 2, 2]               0\n","          Conv2d-144            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-145            [-1, 256, 2, 2]             512\n","            ReLU-146            [-1, 256, 2, 2]               0\n","          Conv2d-147           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-148           [-1, 1024, 2, 2]           2,048\n","            ReLU-149           [-1, 1024, 2, 2]               0\n","      Bottleneck-150           [-1, 1024, 2, 2]               0\n","          Conv2d-151            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-152            [-1, 256, 2, 2]             512\n","            ReLU-153            [-1, 256, 2, 2]               0\n","          Conv2d-154            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-155            [-1, 256, 2, 2]             512\n","            ReLU-156            [-1, 256, 2, 2]               0\n","          Conv2d-157           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-158           [-1, 1024, 2, 2]           2,048\n","            ReLU-159           [-1, 1024, 2, 2]               0\n","      Bottleneck-160           [-1, 1024, 2, 2]               0\n","          Conv2d-161            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-162            [-1, 256, 2, 2]             512\n","            ReLU-163            [-1, 256, 2, 2]               0\n","          Conv2d-164            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-165            [-1, 256, 2, 2]             512\n","            ReLU-166            [-1, 256, 2, 2]               0\n","          Conv2d-167           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-168           [-1, 1024, 2, 2]           2,048\n","            ReLU-169           [-1, 1024, 2, 2]               0\n","      Bottleneck-170           [-1, 1024, 2, 2]               0\n","          Conv2d-171            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-172            [-1, 256, 2, 2]             512\n","            ReLU-173            [-1, 256, 2, 2]               0\n","          Conv2d-174            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-175            [-1, 256, 2, 2]             512\n","            ReLU-176            [-1, 256, 2, 2]               0\n","          Conv2d-177           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-178           [-1, 1024, 2, 2]           2,048\n","            ReLU-179           [-1, 1024, 2, 2]               0\n","      Bottleneck-180           [-1, 1024, 2, 2]               0\n","          Conv2d-181            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-182            [-1, 256, 2, 2]             512\n","            ReLU-183            [-1, 256, 2, 2]               0\n","          Conv2d-184            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-185            [-1, 256, 2, 2]             512\n","            ReLU-186            [-1, 256, 2, 2]               0\n","          Conv2d-187           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-188           [-1, 1024, 2, 2]           2,048\n","            ReLU-189           [-1, 1024, 2, 2]               0\n","      Bottleneck-190           [-1, 1024, 2, 2]               0\n","          Conv2d-191            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-192            [-1, 256, 2, 2]             512\n","            ReLU-193            [-1, 256, 2, 2]               0\n","          Conv2d-194            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-195            [-1, 256, 2, 2]             512\n","            ReLU-196            [-1, 256, 2, 2]               0\n","          Conv2d-197           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-198           [-1, 1024, 2, 2]           2,048\n","            ReLU-199           [-1, 1024, 2, 2]               0\n","      Bottleneck-200           [-1, 1024, 2, 2]               0\n","          Conv2d-201            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-202            [-1, 256, 2, 2]             512\n","            ReLU-203            [-1, 256, 2, 2]               0\n","          Conv2d-204            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-205            [-1, 256, 2, 2]             512\n","            ReLU-206            [-1, 256, 2, 2]               0\n","          Conv2d-207           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-208           [-1, 1024, 2, 2]           2,048\n","            ReLU-209           [-1, 1024, 2, 2]               0\n","      Bottleneck-210           [-1, 1024, 2, 2]               0\n","          Conv2d-211            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-212            [-1, 256, 2, 2]             512\n","            ReLU-213            [-1, 256, 2, 2]               0\n","          Conv2d-214            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-215            [-1, 256, 2, 2]             512\n","            ReLU-216            [-1, 256, 2, 2]               0\n","          Conv2d-217           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n","            ReLU-219           [-1, 1024, 2, 2]               0\n","      Bottleneck-220           [-1, 1024, 2, 2]               0\n","          Conv2d-221            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-222            [-1, 256, 2, 2]             512\n","            ReLU-223            [-1, 256, 2, 2]               0\n","          Conv2d-224            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-225            [-1, 256, 2, 2]             512\n","            ReLU-226            [-1, 256, 2, 2]               0\n","          Conv2d-227           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-228           [-1, 1024, 2, 2]           2,048\n","            ReLU-229           [-1, 1024, 2, 2]               0\n","      Bottleneck-230           [-1, 1024, 2, 2]               0\n","          Conv2d-231            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-232            [-1, 256, 2, 2]             512\n","            ReLU-233            [-1, 256, 2, 2]               0\n","          Conv2d-234            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-235            [-1, 256, 2, 2]             512\n","            ReLU-236            [-1, 256, 2, 2]               0\n","          Conv2d-237           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-238           [-1, 1024, 2, 2]           2,048\n","            ReLU-239           [-1, 1024, 2, 2]               0\n","      Bottleneck-240           [-1, 1024, 2, 2]               0\n","          Conv2d-241            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-242            [-1, 256, 2, 2]             512\n","            ReLU-243            [-1, 256, 2, 2]               0\n","          Conv2d-244            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-245            [-1, 256, 2, 2]             512\n","            ReLU-246            [-1, 256, 2, 2]               0\n","          Conv2d-247           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-248           [-1, 1024, 2, 2]           2,048\n","            ReLU-249           [-1, 1024, 2, 2]               0\n","      Bottleneck-250           [-1, 1024, 2, 2]               0\n","          Conv2d-251            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-252            [-1, 256, 2, 2]             512\n","            ReLU-253            [-1, 256, 2, 2]               0\n","          Conv2d-254            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-255            [-1, 256, 2, 2]             512\n","            ReLU-256            [-1, 256, 2, 2]               0\n","          Conv2d-257           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-258           [-1, 1024, 2, 2]           2,048\n","            ReLU-259           [-1, 1024, 2, 2]               0\n","      Bottleneck-260           [-1, 1024, 2, 2]               0\n","          Conv2d-261            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-262            [-1, 256, 2, 2]             512\n","            ReLU-263            [-1, 256, 2, 2]               0\n","          Conv2d-264            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-265            [-1, 256, 2, 2]             512\n","            ReLU-266            [-1, 256, 2, 2]               0\n","          Conv2d-267           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-268           [-1, 1024, 2, 2]           2,048\n","            ReLU-269           [-1, 1024, 2, 2]               0\n","      Bottleneck-270           [-1, 1024, 2, 2]               0\n","          Conv2d-271            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-272            [-1, 256, 2, 2]             512\n","            ReLU-273            [-1, 256, 2, 2]               0\n","          Conv2d-274            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-275            [-1, 256, 2, 2]             512\n","            ReLU-276            [-1, 256, 2, 2]               0\n","          Conv2d-277           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-278           [-1, 1024, 2, 2]           2,048\n","            ReLU-279           [-1, 1024, 2, 2]               0\n","      Bottleneck-280           [-1, 1024, 2, 2]               0\n","          Conv2d-281            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-282            [-1, 256, 2, 2]             512\n","            ReLU-283            [-1, 256, 2, 2]               0\n","          Conv2d-284            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-285            [-1, 256, 2, 2]             512\n","            ReLU-286            [-1, 256, 2, 2]               0\n","          Conv2d-287           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-288           [-1, 1024, 2, 2]           2,048\n","            ReLU-289           [-1, 1024, 2, 2]               0\n","      Bottleneck-290           [-1, 1024, 2, 2]               0\n","          Conv2d-291            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-292            [-1, 256, 2, 2]             512\n","            ReLU-293            [-1, 256, 2, 2]               0\n","          Conv2d-294            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-295            [-1, 256, 2, 2]             512\n","            ReLU-296            [-1, 256, 2, 2]               0\n","          Conv2d-297           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-298           [-1, 1024, 2, 2]           2,048\n","            ReLU-299           [-1, 1024, 2, 2]               0\n","      Bottleneck-300           [-1, 1024, 2, 2]               0\n","          Conv2d-301            [-1, 256, 2, 2]         262,144\n","     BatchNorm2d-302            [-1, 256, 2, 2]             512\n","            ReLU-303            [-1, 256, 2, 2]               0\n","          Conv2d-304            [-1, 256, 2, 2]         589,824\n","     BatchNorm2d-305            [-1, 256, 2, 2]             512\n","            ReLU-306            [-1, 256, 2, 2]               0\n","          Conv2d-307           [-1, 1024, 2, 2]         262,144\n","     BatchNorm2d-308           [-1, 1024, 2, 2]           2,048\n","            ReLU-309           [-1, 1024, 2, 2]               0\n","      Bottleneck-310           [-1, 1024, 2, 2]               0\n","          Conv2d-311            [-1, 512, 2, 2]         524,288\n","     BatchNorm2d-312            [-1, 512, 2, 2]           1,024\n","            ReLU-313            [-1, 512, 2, 2]               0\n","          Conv2d-314            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-315            [-1, 512, 1, 1]           1,024\n","            ReLU-316            [-1, 512, 1, 1]               0\n","          Conv2d-317           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-318           [-1, 2048, 1, 1]           4,096\n","          Conv2d-319           [-1, 2048, 1, 1]       2,097,152\n","     BatchNorm2d-320           [-1, 2048, 1, 1]           4,096\n","            ReLU-321           [-1, 2048, 1, 1]               0\n","      Bottleneck-322           [-1, 2048, 1, 1]               0\n","          Conv2d-323            [-1, 512, 1, 1]       1,048,576\n","     BatchNorm2d-324            [-1, 512, 1, 1]           1,024\n","            ReLU-325            [-1, 512, 1, 1]               0\n","          Conv2d-326            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-327            [-1, 512, 1, 1]           1,024\n","            ReLU-328            [-1, 512, 1, 1]               0\n","          Conv2d-329           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-330           [-1, 2048, 1, 1]           4,096\n","            ReLU-331           [-1, 2048, 1, 1]               0\n","      Bottleneck-332           [-1, 2048, 1, 1]               0\n","          Conv2d-333            [-1, 512, 1, 1]       1,048,576\n","     BatchNorm2d-334            [-1, 512, 1, 1]           1,024\n","            ReLU-335            [-1, 512, 1, 1]               0\n","          Conv2d-336            [-1, 512, 1, 1]       2,359,296\n","     BatchNorm2d-337            [-1, 512, 1, 1]           1,024\n","            ReLU-338            [-1, 512, 1, 1]               0\n","          Conv2d-339           [-1, 2048, 1, 1]       1,048,576\n","     BatchNorm2d-340           [-1, 2048, 1, 1]           4,096\n","            ReLU-341           [-1, 2048, 1, 1]               0\n","      Bottleneck-342           [-1, 2048, 1, 1]               0\n","AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n","          Linear-344                   [-1, 10]          20,490\n","================================================================\n","Total params: 42,512,970\n","Trainable params: 42,512,970\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 8.79\n","Params size (MB): 162.17\n","Estimated Total Size (MB): 170.97\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["learning_rate = 1e-3\n","momentum=0.9\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"DsKsyM8MaLgv","executionInfo":{"status":"ok","timestamp":1642339796931,"user_tz":-540,"elapsed":299,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["# scheduler = torch.optim.lr_scheduler.LambdaLR(\n","#     optimizer=optimizer, \n","#     lr_lambda=lambda epoch: 0.95**epoch, \n","#     last_epoch=-1, \n","#     verbose=True)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, verbose=True, mode='max', patience=5)\n","print(optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DdrChWdPDh1","executionInfo":{"status":"ok","timestamp":1642339798467,"user_tz":-540,"elapsed":279,"user":{"displayName":"Sohyun An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiA5fbxTFul4oSf0J-aANYw-cKEXK7BcQfHqodu=s64","userId":"03040571318498638819"}},"outputId":"59a3e039-b3ad-4919-d674-536c53b3b5eb"},"execution_count":175,"outputs":[{"output_type":"stream","name":"stdout","text":["SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.001\n","    momentum: 0.9\n","    nesterov: False\n","    weight_decay: 0\n",")\n"]}]},{"cell_type":"code","source":["train(model, optimizer, scheduler, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQhK2qh97ldj","outputId":"6f446c0d-31f5-4e94-b775-e5bd04023067"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------  Epoch: 1  --------------------\n","Epoch 1\n","Iteration 0, loss = 2.3484\n","Checking accuracy on validation set\n","Got 81 / 1000 correct (8.10)\n","\n","Epoch 1\n","Iteration 100, loss = 1.2776\n","Checking accuracy on validation set\n","Got 566 / 1000 correct (56.60)\n","\n","Epoch 1\n","Iteration 200, loss = 0.8850\n","Checking accuracy on validation set\n","Got 756 / 1000 correct (75.60)\n","\n","Epoch 1\n","Iteration 300, loss = 0.4944\n","Checking accuracy on validation set\n","Got 814 / 1000 correct (81.40)\n","\n","Epoch 1\n","Iteration 400, loss = 0.4195\n","Checking accuracy on validation set\n","Got 843 / 1000 correct (84.30)\n","\n","Epoch 1\n","Iteration 500, loss = 0.3590\n","Checking accuracy on validation set\n","Got 875 / 1000 correct (87.50)\n","\n","Epoch 1\n","Iteration 600, loss = 0.3756\n","Checking accuracy on validation set\n","Got 898 / 1000 correct (89.80)\n","\n","Epoch 1\n","Iteration 700, loss = 0.3423\n","Checking accuracy on validation set\n","Got 915 / 1000 correct (91.50)\n","\n","--------------------  Epoch: 2  --------------------\n","Epoch 2\n","Iteration 0, loss = 0.2445\n","Checking accuracy on validation set\n","Got 927 / 1000 correct (92.70)\n","\n","Epoch 2\n","Iteration 100, loss = 0.2388\n","Checking accuracy on validation set\n","Got 927 / 1000 correct (92.70)\n","\n","Epoch 2\n","Iteration 200, loss = 0.3044\n","Checking accuracy on validation set\n","Got 929 / 1000 correct (92.90)\n","\n","Epoch 2\n","Iteration 300, loss = 0.2475\n","Checking accuracy on validation set\n","Got 944 / 1000 correct (94.40)\n","\n","Epoch 2\n","Iteration 400, loss = 0.1695\n","Checking accuracy on validation set\n","Got 931 / 1000 correct (93.10)\n","\n"]}]},{"cell_type":"code","source":["best_model = model\n","test_acc = check_accuracy(loader_test, best_model)"],"metadata":{"id":"QXV9ZTgM7_Wm"},"execution_count":null,"outputs":[]}]}